{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import Row, SparkSession\n",
    "\n",
    "from pyspark.sql.functions import regexp_extract\n",
    "\n",
    "# Create a SparkSession (the config bit is only for Windows!)\n",
    "spark = SparkSession.builder.config(\"spark.sql.warehouse.dir\").appName(\"StructuredStreaming\").getOrCreate()\n",
    "\n",
    "# Monitor the logs directory for new log data, and read in the raw lines as accessLines\n",
    "accessLines = spark.readStream.text(\"logs\")\n",
    "\n",
    "# Parse out the common log format to a DataFrame\n",
    "contentSizeExp = r'\\s(\\d+)$'\n",
    "statusExp = r'\\s(\\d{3})\\s'\n",
    "generalExp = r'\\\"(\\S+)\\s(\\S+)\\s*(\\S*)\\\"'\n",
    "timeExp = r'\\[(\\d{2}/\\w{3}/\\d{4}:\\d{2}:\\d{2}:\\d{2} -\\d{4})]'\n",
    "hostExp = r'(^\\S+\\.[\\S+\\.]+\\S+)\\s'\n",
    "\n",
    "logsDF = accessLines.select(regexp_extract('value', hostExp, 1).alias('host'),\n",
    "                         regexp_extract('value', timeExp, 1).alias('timestamp'),\n",
    "                         regexp_extract('value', generalExp, 1).alias('method'),\n",
    "                         regexp_extract('value', generalExp, 2).alias('endpoint'),\n",
    "                         regexp_extract('value', generalExp, 3).alias('protocol'),\n",
    "                         regexp_extract('value', statusExp, 1).cast('integer').alias('status'),\n",
    "                         regexp_extract('value', contentSizeExp, 1).cast('integer').alias('content_size'))\n",
    "\n",
    "# Keep a running count of every access by status code\n",
    "statusCountsDF = logsDF.groupBy(logsDF.status).count()\n",
    "\n",
    "# Kick off our streaming query, dumping results to the console\n",
    "query = ( statusCountsDF.writeStream.outputMode(\"complete\").format(\"console\").queryName(\"counts\").start() )\n",
    "\n",
    "# Run forever until terminated\n",
    "query.awaitTermination()\n",
    "\n",
    "# Cleanly shut down the session\n",
    "spark.stop()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
